{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ],
      "metadata": {
        "id": "3eBS7HnfRH2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ],
      "metadata": {
        "id": "SOIanr_0RLVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3EutJPfVTmq"
      },
      "outputs": [],
      "source": [
        "!pip install Levenshtein\n",
        "!pip install weighted-levenshtein\n",
        "!pip install qwerty-weighted-levenshtein\n",
        "!pip install typo\n",
        "import re\n",
        "import typo\n",
        "from Levenshtein import distance as LDist\n",
        "from qwerty_weighted_levenshtein import qwerty_weighted_levenshtein as qDist\n",
        "from tqdm import tqdm\n",
        "from math import log\n",
        "from random import choice, sample\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "DHd_EMi0WGKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "12223eeb-2509-4450-dd89-ed51c85ce2bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/big.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-b7e472344bd0>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/big.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtext_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtext_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_full\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/big.txt'"
          ]
        }
      ],
      "source": [
        "# Helper functions & reading the data\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(string):\n",
        "    string = string.lower() # lowercases\n",
        "    string = re.sub(r\"[^\\w\\d'\\s]+\",' ',string) # removes punctuation and special characters\n",
        "    string = re.sub(r\"[0-9]\", \"\", string) # removes digits\n",
        "    string = ' '.join(string.split()) # removes accidental spaces\n",
        "    return string\n",
        "\n",
        "\n",
        "def expand(x, k=None):\n",
        "    if type(x) in [list, set]:\n",
        "        [print(item) for item in (x if k is None else x[:k])]\n",
        "    if type(x) == dict:\n",
        "        [print(k, v) for k, v in (x.items() if k is None else list(x.items()[:k]))]\n",
        "\n",
        "\n",
        "# Opens and reads the file with data\n",
        "with open(\"/big.txt\", \"r\") as f:\n",
        "    text_full = f.readlines()\n",
        "    text_full = [preprocess_text(x[:-1]) for x in text_full]\n",
        "    text_full = [x for x in text_full if 50 >= len(x.split()) >= 5]\n",
        "\n",
        "# Train/test random split\n",
        "test_idx = sample(range(len(text_full)), 58)\n",
        "train_idx = list(set(range(len(text_full))) - set(test_idx))\n",
        "text_test = [text_full[i] for i in test_idx]\n",
        "text_train = [text_full[i] for i in train_idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(string, center, rng):\n",
        "    \"\"\"\n",
        "    This is a helper function that takes a list and in index\n",
        "    and returns a list of this index's word's neighbors (aka context)\n",
        "    \"\"\"\n",
        "    return string[max(center-rng,0):center] + string[center+1:min(center+rng+1, len(string))]\n",
        "\n",
        "\n",
        "def build_wwm(string_list, context_len=3):\n",
        "    \"\"\"\n",
        "    Helper function; takes a list of strings\n",
        "    and returns vocabulary and word context matrix\n",
        "    \"\"\"\n",
        "    wwm = dict()   # word context matrix\n",
        "    vocab = dict() # vocabulary count\n",
        "    cont = dict()  # context count\n",
        "    total = 0  # total n words count\n",
        "\n",
        "    for sent in string_list:\n",
        "        # processes each word in string\n",
        "        sent = preprocess_text(sent).split()\n",
        "        for i in range(len(sent)):\n",
        "            # gets the word's context\n",
        "            context = get_context(sent, i, context_len)\n",
        "            for word in context:\n",
        "                # updates context matrix\n",
        "                temp = wwm.get(sent[i], dict())\n",
        "                temp[word] = temp.get(word, 10) + 1\n",
        "                wwm[sent[i]] = temp\n",
        "\n",
        "                cont[word] = cont.get(word, 10) + 1\n",
        "                total += 1\n",
        "            vocab[sent[i]] = vocab.get(sent[i], 10) + 1\n",
        "\n",
        "    # calculates PPMI for each value\n",
        "    for k, v in wwm.items():\n",
        "        for k1, v1 in v.items():\n",
        "            v[k1] = max(0, log((v1*total)/(vocab[k]*cont[k1]), 2))\n",
        "        wwm[k] = v\n",
        "\n",
        "    return wwm, vocab, cont\n",
        "\n",
        "\n",
        "wwm, vocab, context = build_wwm(text_train)\n",
        "\n",
        "def get_candidates(word, m, k):\n",
        "    \"\"\"\n",
        "    Helper function. Takes a word and word matrix\n",
        "    and returns k most similar words\n",
        "    \"\"\"\n",
        "    candidates, dist_thres = list(), float(\"inf\")\n",
        "    for item in m.keys():\n",
        "        dist = qDist(word, item)\n",
        "        if dist <= dist_thres:\n",
        "            candidates.append([item, dist])\n",
        "            dist_thres = min(dist_thres, max(3, dist))\n",
        "\n",
        "    candidates.sort(key=lambda l:l[1])\n",
        "    return candidates[:k]\n",
        "\n",
        "\n",
        "def predict(word, word_candidates, context_replaced, wwm, voc):\n",
        "    \"\"\"\n",
        "    Helper function -- takes a word, its candidates, context, word and context matricies\n",
        "    and makes prediction for a single word\n",
        "    \"\"\"\n",
        "    candidates = list()\n",
        "    for i, candidate in enumerate(word_candidates):\n",
        "        count = 1\n",
        "        for context_word in context_replaced:\n",
        "            count += wwm[candidate[0]].get(context_word[0], 0)\n",
        "        candidates.append([candidate[0], count/(4**(candidate[1]-1.7))])\n",
        "\n",
        "    return max(candidates, key=lambda l:l[1])[0]\n",
        "\n",
        "\n",
        "def autocorrect(sentence, wwm=wwm, voc=vocab):\n",
        "    \"\"\"\n",
        "    Top-level function for string spelling correction.\n",
        "    It takes a string and returns a transformed version of it with mistakes corrected.\n",
        "\n",
        "    Parameters:\n",
        "    sentence (str): Text that needs to be corrected\n",
        "    wwm (dict): A word-context matrix (do not touch it; it is set by default)\n",
        "    voc (dict): Vocabulary matrix (do not touch it too)\n",
        "\n",
        "    Returns:\n",
        "    (str): corrected version of the provided text.\n",
        "\n",
        "    \"\"\"\n",
        "    sentence = preprocess_text(sentence).split()\n",
        "    # Gets candidates for all words in the sentence\n",
        "    all_candidates = [get_candidates(x, wwm, 30) for x in sentence]\n",
        "    # Gets the context version of the text\n",
        "    sentence_substitutes = [x[0] for x in all_candidates]\n",
        "\n",
        "    # Makes a prediction for each word and adds it to the new sentence\n",
        "    new_sentence = list()\n",
        "    for i in range(len(sentence)):\n",
        "        new_sentence.append(predict(sentence[i], all_candidates[i], get_context(sentence_substitutes, i, 2), wwm, voc))\n",
        "\n",
        "    # Joins the resulting list and returns\n",
        "    return ' '.join(new_sentence)"
      ],
      "metadata": {
        "id": "xutHjWgc26XU"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justify your decisions\n",
        "\n",
        "I used Norvig's 'big.txt' file to generate my own dataset for this problem.\n",
        "\n",
        "The algorithm works as follows: it analyses each word and its context (by default, the context is 2 word before and 2 words after the given word). As a result, I have a dictionary of words and its neighboring words. When performing autocorrection, each words is given a list of candidates, and then the best candidate is chosen based on its context in the sentence and the learnt context words.\n",
        "\n",
        "Norvig's and my algorithms are very similar because the logic behind them is very similar -- they find the most similar and frequent word from the known vocabulary and replace \"unseen\" words with \"correct\" ones. The idea of my improvement is to make this process more flexible by adding the ability to also consider neighboring words when making this decision (as opposed to simply choosing the most common word).\n",
        "\n",
        "This way I can achieve several goals:\n",
        "- Better robustness. Although the technique I used is not exactly a classic n-gram model, it allows for more flexibility in case of unseen \"n-grams\" and does a better work generalizing and analyzing the context and meanings.\n",
        "- Context-sensitivity. The model learns the context of each word, and when making predictions, it chooses the candidate with the best context match.\n",
        "- A potential ability to change correct but misused words. Since the model takes the context into account, it may change a misused word into a more appropriate but similar word, for example compare 'you\"re' vs 'your'.\n",
        "\n",
        "The candidates for each word are based on similarity -- Levenshtein distance (for evaluation) and Damerau-Levenshtein distance (for predicting), thus taking into account the mistakes associated with the keyboard layout.\n",
        "\n",
        "One of the biggest downsides is this algorithm doesn't account for missing and extra spaces. Due to this assignment's limitation it is impossible to detect such cases, and iterating through each character and space to test for them is too time consuming.\n",
        "\n",
        "\n",
        "As for evaluation, I take a valid sentence and make realistic looking mistakes: it randomly swaps two characters, replaces a character with its neighbour on a qwerty keyboard, adds/removes random characters (except for spaces) and so on. The number of such replacements is regulated with the noise parameter. Finally, it compares the noisy and corrected sentences with the initial valid one."
      ],
      "metadata": {
        "id": "AIOhV2oyRS2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ],
      "metadata": {
        "id": "2ixCdllURYqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of context sensitivity\n",
        "\n",
        "print(autocorrect(\"dyad silk dress\"))\n",
        "print(autocorrect(\"He is already dyad\"), \"\\n\")\n",
        "\n",
        "print(autocorrect(\"Are you dking sports\"))\n",
        "print(autocorrect(\"He is dking of fright!\")) # there's no word 'species' in big.txt (:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DT-nszsDC9f",
        "outputId": "89f5861f-6c44-4766-cd7c-7faa465bdaea"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dyed silk dress\n",
            "he is already dead \n",
            "\n",
            "are you doing sports\n",
            "he is dying of fright\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator():\n",
        "    \"\"\"\n",
        "    This class provides an interface for evaluating the model performance.\n",
        "    It takes a list of strings, applies some noise, calls the correction function,\n",
        "    and evaluates the result using Levenshtein distance\n",
        "\n",
        "    Parameters:\n",
        "    text_data (List[str]): A collection of texts to perform tests on\n",
        "    corrector (Callable): Your spelling correction function\n",
        "    show_examples (Bool): If true, shows 5 examples of the corrector's output;\n",
        "                          if false, it evaluates the performance of the entire text_data\n",
        "                          without showing any text results of the corrector\n",
        "    noise (float): Shows how many mistakes to generate. 0 means no mistakes, and the bigger the value, the more mistakes\n",
        "    noisy_data (list): Kostyl'; used for fair comparison between algorithms (do not touch it !!)\n",
        "    \"\"\"\n",
        "    def __init__(self, text_data, corrector, show_examples, noise, noisy_data=None):\n",
        "        self.data = text_data\n",
        "        self.corrector = corrector\n",
        "        self.show_examples = show_examples\n",
        "        self.noise = noise\n",
        "\n",
        "        # cleaning text\n",
        "        self.data = [preprocess_text(x) for x in self.data]\n",
        "        # generating text with typos\n",
        "        self.noisy_data = [self.create_errors(x) for x in self.data] if noisy_data is None else noisy_data\n",
        "        # evaluation\n",
        "        self.evaluate()\n",
        "\n",
        "\n",
        "    def create_errors(self, string):\n",
        "        \"\"\"\n",
        "        Helper function; adds realistic-looking mistakes to a given string\n",
        "        \"\"\"\n",
        "        errer = typo.StrErrer(string)\n",
        "        for i in range(int(len(string)*self.noise)):\n",
        "            fun = choice([errer.missing_char, errer.extra_char, errer.nearby_char,\n",
        "                          errer.similar_char, errer.repeated_char, errer.unichar])\n",
        "            string = fun().result #very fun result\n",
        "        return string\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Main function that calls after this class's init function.\n",
        "        Applies the corrector function to sentences with mistakes and prints the report\n",
        "        \"\"\"\n",
        "        # if show_example is True, then it just prints 5 lines of sentences\n",
        "        if self.show_examples == True:\n",
        "            for i in range(min(5, len(self.data))):\n",
        "                sent = self.data[i]\n",
        "                noisy_sent = self.noisy_data[i]\n",
        "                corrected_sent = self.corrector(noisy_sent)\n",
        "\n",
        "                # Calculates the Levenshtein distances between correct & noisy and correct & corrected sentences\n",
        "                dist_init = LDist(sent, noisy_sent)\n",
        "                dist_corrected = LDist(sent, corrected_sent)\n",
        "\n",
        "                print(f\"Sent #{i+1} --- Length: {len(sent)}  # Mistakes initially: {dist_init}   # Mistakes in corrected: {dist_corrected}\")\n",
        "                print(\"    TRUTH\", sent)\n",
        "                print(\"    NOISY\", noisy_sent)\n",
        "                print(\"CORRECTED\", corrected_sent)\n",
        "                print()\n",
        "\n",
        "\n",
        "        # if show_example is False, then it evaluates the entire dataset\n",
        "        if self.show_examples == False:\n",
        "            # Some values for statistics\n",
        "            mistakes_init_count, mistakes_count, total_length_count = 0, 0, 0\n",
        "\n",
        "            # Initilaizes a progress bar\n",
        "            pbar = tqdm(total=len(self.data), position=0, leave=True)\n",
        "            pbar.set_description_str(\"EVALUATING\")\n",
        "\n",
        "            for i in range(len(self.data)):\n",
        "                sent = self.data[i]\n",
        "                noisy_sent = self.noisy_data[i]\n",
        "                corrected_sent = self.corrector(noisy_sent)\n",
        "\n",
        "                # Calculates the Levenshtein distances between correct & noisy and correct & corrected sentences\n",
        "                dist_init = LDist(sent, noisy_sent)\n",
        "                dist_corrected = LDist(sent, corrected_sent)\n",
        "\n",
        "                # Updates the counters\n",
        "                total_length_count += len(sent)\n",
        "                mistakes_init_count += dist_init\n",
        "                mistakes_count += dist_corrected\n",
        "\n",
        "                # Updates the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix_str(f\"Noisy sentence accuracy: {round(100-100*mistakes_init_count/total_length_count, 3)}%   Corrected sentence accuracy: {round(100-100*mistakes_count/total_length_count, 3)}%\")"
      ],
      "metadata": {
        "id": "WyaQi0KV3Cpz"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements the Norvig's algorithm (with slight changes) (for comparison)\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(' '.join(text_train)))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "metadata": {
        "id": "9FZK57JLjgSz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the output of my and Norvig's solutions\n",
        "\n",
        "print(\"EXAMPLES OF MY SOLUTION\")\n",
        "noisy = Evaluator(text_test, autocorrect, True, noise=0.1).noisy_data\n",
        "\n",
        "print(\"\\n\\n\\nEXAMPLES OF NORVIG'S SOLUTION\")\n",
        "norvig = lambda l: ' '.join(correction(x) for x in l.split())\n",
        "Evaluator(text_test, norvig, True, 0.1, noisy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPv2Uhc02-Up",
        "outputId": "8f91e8cc-0d26-4728-f85d-bc422e82e661"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXAMPLES OF MY SOLUTION\n",
            "Sent #1 --- Length: 63  # Mistakes initially: 4   # Mistakes in corrected: 0\n",
            "    TRUTH updated editions will replace the previous one the old editions\n",
            "    NOISY updated ediytions will replace the preuious one the old edtiohns\n",
            "CORRECTED updated editions will replace the previous one the old editions\n",
            "\n",
            "Sent #2 --- Length: 70  # Mistakes initially: 5   # Mistakes in corrected: 0\n",
            "    TRUTH several varieties of whitlow are recognised but while it is convenient\n",
            "    NOISY sevedral varicties of whitlow are recoggnised but whilc it is convenicnt\n",
            "CORRECTED several varieties of whitlow are recognised but while it is convenient\n",
            "\n",
            "Sent #3 --- Length: 64  # Mistakes initially: 6   # Mistakes in corrected: 1\n",
            "    TRUTH realize the significance of his appearance before her now prince\n",
            "    NOISY realize the significancce of his apprearwndce befire her now price\n",
            "CORRECTED realize the significance of his appearance before her now price\n",
            "\n",
            "Sent #4 --- Length: 65  # Mistakes initially: 6   # Mistakes in corrected: 0\n",
            "    TRUTH had taken a life another had taken two a third had set a house on\n",
            "    NOISY had taken a olife anotuer haed takeen twl a third had set a yhouse on\n",
            "CORRECTED had taken a life another had taken two a third had set a house on\n",
            "\n",
            "Sent #5 --- Length: 28  # Mistakes initially: 2   # Mistakes in corrected: 0\n",
            "    TRUTH elephantiasis in a woman aet\n",
            "    NOISY elepantiasis in a womab aet\n",
            "CORRECTED elephantiasis in a woman aet\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "EXAMPLES OF NORVIG'S SOLUTION\n",
            "Sent #1 --- Length: 63  # Mistakes initially: 4   # Mistakes in corrected: 0\n",
            "    TRUTH updated editions will replace the previous one the old editions\n",
            "    NOISY updated ediytions will replace the preuious one the old edtiohns\n",
            "CORRECTED updated editions will replace the previous one the old editions\n",
            "\n",
            "Sent #2 --- Length: 70  # Mistakes initially: 5   # Mistakes in corrected: 0\n",
            "    TRUTH several varieties of whitlow are recognised but while it is convenient\n",
            "    NOISY sevedral varicties of whitlow are recoggnised but whilc it is convenicnt\n",
            "CORRECTED several varieties of whitlow are recognised but while it is convenient\n",
            "\n",
            "Sent #3 --- Length: 64  # Mistakes initially: 6   # Mistakes in corrected: 4\n",
            "    TRUTH realize the significance of his appearance before her now prince\n",
            "    NOISY realize the significancce of his apprearwndce befire her now price\n",
            "CORRECTED realize the significance of his apprearwndce before her now price\n",
            "\n",
            "Sent #4 --- Length: 65  # Mistakes initially: 6   # Mistakes in corrected: 0\n",
            "    TRUTH had taken a life another had taken two a third had set a house on\n",
            "    NOISY had taken a olife anotuer haed takeen twl a third had set a yhouse on\n",
            "CORRECTED had taken a life another had taken two a third had set a house on\n",
            "\n",
            "Sent #5 --- Length: 28  # Mistakes initially: 2   # Mistakes in corrected: 0\n",
            "    TRUTH elephantiasis in a woman aet\n",
            "    NOISY elepantiasis in a womab aet\n",
            "CORRECTED elephantiasis in a woman aet\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Evaluator at 0x7e12f4e84ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPARISON\n",
        "\n",
        "for noise in [0.05, 0.1, 0.15]:\n",
        "    print(\"My solution, noise:\", noise)\n",
        "    noisy = Evaluator(text_test, autocorrect, False, noise).noisy_data\n",
        "\n",
        "    print(\"\\nNorvig's solution, noise:\", noise)\n",
        "    Evaluator(text_test, norvig, False, noise, noisy)\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKCFl14oR8aT",
        "outputId": "733406ee-07dc-4fc8-8cef-a3f109f8f606"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My solution, noise: 0.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:49<00:00,  1.18it/s, Noisy sentence accuracy: 96.256%   Corrected sentence accuracy: 98.743%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Norvig's solution, noise: 0.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:01<00:00, 50.49it/s, Noisy sentence accuracy: 96.256%   Corrected sentence accuracy: 98.771%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "My solution, noise: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:50<00:00,  1.16it/s, Noisy sentence accuracy: 93.155%   Corrected sentence accuracy: 97.29%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Norvig's solution, noise: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:04<00:00, 14.50it/s, Noisy sentence accuracy: 93.155%   Corrected sentence accuracy: 97.15%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "My solution, noise: 0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:50<00:00,  1.16it/s, Noisy sentence accuracy: 90.053%   Corrected sentence accuracy: 96.032%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Norvig's solution, noise: 0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING: 100%|██████████| 58/58 [00:08<00:00,  6.77it/s, Noisy sentence accuracy: 90.053%   Corrected sentence accuracy: 95.613%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norvig's and my solution show similar results\n",
        "\n",
        "If there are few mistakes, Norvig's algorithm tends to have better and faster results, but its performance and accuracy drops as sentences become more noisy.\n",
        "\n",
        "For the case when noise=0.05, both algorithms import the accuracy by +2.5%; for moderately noisy sentences, by +4.1%. Finally, if there is a lot of mistakes, Norvig makes an improvement by +5.6% and I by +6%.\n",
        "\n",
        "I may explain that in my algorithm time complexity depends on the number of words, so the time is the same in all cases, while Norvig has implemented a \"bruteforce\" solution to find the candidates, so more mistakes mean more candidates to choose from.\n",
        "\n",
        "But both algorithms are essentially similar, because the main logic is to choose most common and \"seen\" words, and the idea of my improvement is to make this process a little more flexible by adding the notion of context."
      ],
      "metadata": {
        "id": "_5szvHmrmpvB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
