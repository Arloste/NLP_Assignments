{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & reading the data"
      ],
      "metadata": {
        "id": "fErtSkFGvq_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ast2lXhtvZy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08850ee8-a3f7-44e4-9d66-734955ec1764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.7.0/ru_core_news_lg-3.7.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.7.0) (3.7.4)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.7.0)\n",
            "  Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m761.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.7.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.7.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.1.5)\n",
            "Installing collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from prettytable import PrettyTable\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "!python -m spacy download ru_core_news_lg\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"../train.jsonl\") as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "# sentences and ners from `train.jsonl`\n",
        "train_sent = list()\n",
        "train_ners = list()\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    train_sent.append(result[\"sentences\"])\n",
        "\n",
        "    train_ners.append(\n",
        "        sorted(\n",
        "            sorted([(result[\"sentences\"][start:end+1], start, end, t) for start, end, t in result[\"ners\"]], key=lambda l:l[2], reverse=True),\n",
        "            key=lambda l: l[1]\n",
        "        )\n",
        "    )\n",
        "\n",
        "with open(\"../test.jsonl\") as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "# sentences and corresponding ids from `test.jsonl`\n",
        "test_sent = list()\n",
        "test_idx = list()\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    test_sent.append(result[\"senences\"]) # sEnENceS\n",
        "    test_idx.append(result[\"id\"])"
      ],
      "metadata": {
        "id": "_qFl8SXFwI11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model code"
      ],
      "metadata": {
        "id": "i6dvwJvevw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities_spacy(sent, recursion_depth=6):\n",
        "    \"\"\"\n",
        "    This function takes a sentence and a maximum recursion depth\n",
        "    and returns a list of all found entities without duplicates.\n",
        "\n",
        "    Arguments:\n",
        "        sent (str): a string where we need to find the entities\n",
        "        recursion_depth (int): maximum recursion depth we may call\n",
        "\n",
        "    Returns:\n",
        "        List(Tuple(str, int, int, str)): a list of all found entities\n",
        "                                         in a format (<word>, <start idx>, <end idx inclusive>, <label>)\n",
        "    \"\"\"\n",
        "    # stores entities from current sentence\n",
        "    res = list()\n",
        "    doc = nlp(sent)\n",
        "\n",
        "    # stores entities found from sub-calls\n",
        "    sub_ents = list()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        # gets word boundaries\n",
        "        a, b = ent.start_char, ent.end_char\n",
        "        # stores the result in res\n",
        "        res.append((doc.text[a:b], a, b-1, ent.label_))\n",
        "\n",
        "        if recursion_depth and res[-1][0] != sent and len(res[-1][0].split())>1:\n",
        "            # makes a recursive call for each found entity\n",
        "            sub_ent = get_entities_spacy(res[-1][0], recursion_depth-1)\n",
        "\n",
        "            # writes the found entity into sub_ents\n",
        "            sub_ent = [(x[0], x[1]+a, x[2]+a, x[3]) for x in sub_ent]\n",
        "            sub_ents += sub_ent\n",
        "\n",
        "    # returns a list of all unique entities in this call and all sub-calls\n",
        "    return list(set(res + sub_ents))"
      ],
      "metadata": {
        "id": "KuXyDo0kwOWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset splitting"
      ],
      "metadata": {
        "id": "YhRRyqMhvzo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fun(text, ners):\n",
        "    \"\"\"\n",
        "    Unfolds all nested entities recursively.\n",
        "\n",
        "    For example, an input:\n",
        "\n",
        "    \"Ilya works in Moscow University\" -> [\"Ilya\", \"Person\"], [\"Moscow University\", \"Organization\"], [\"Moscow\", \"City\"]\n",
        "\n",
        "    would be split into two separate training examples:\n",
        "\n",
        "    \"Ilya works in Moscow University\" -> [\"Ilya\", \"Person\"], [\"Moscow University\", \"Organization\"]\n",
        "    \"Moscow University\" -> [\"Moscow\", \"City\"]\n",
        "\n",
        "    Parameters:\n",
        "        text (str): a full sentence to be split\n",
        "        ners (List(Tuple(str, int, int, str))): a list of entities to be split\n",
        "\n",
        "    Returns:\n",
        "        List(Tuple(str, List(Tuple(str, int, int, str)))): a list of unfolded entities in a format (<sentence>, <list of ners>)\n",
        "    \"\"\"\n",
        "    # sorts the entities by end idx and start idx\n",
        "    ners.sort(reverse=True, key=lambda l:l[2])\n",
        "    ners.sort(key=lambda l:l[1])\n",
        "\n",
        "    res = []\n",
        "\n",
        "    # shows whether this entity is nested\n",
        "    included = [False] * len(ners)\n",
        "    # stores parent text and location for nested values\n",
        "    par_sent_loc = text\n",
        "    shift_loc = 0\n",
        "\n",
        "    # a dict that stores all nested entities and their parent text\n",
        "    to_process = dict()\n",
        "\n",
        "    for i, item in enumerate(ners):\n",
        "        # if entity is not nested, then store its text and position\n",
        "        if not res or item[1] > res[-1][2]:\n",
        "            res.append(item)\n",
        "            included[i] = True\n",
        "            par_sent_loc = item[0]\n",
        "            shift_loc = item[1]\n",
        "\n",
        "        # if entity is nested, stores it in `to_process`\n",
        "        if not included[i]:\n",
        "            l = to_process.get(par_sent_loc, list())\n",
        "            l.append((item[0], item[1]-shift_loc, item[2]-shift_loc, item[3]))\n",
        "            l = list(set(l))\n",
        "            to_process[par_sent_loc] = l\n",
        "\n",
        "    answer = [[text, res]]\n",
        "\n",
        "    # recursively calls this function for each nested entity\n",
        "    for k, v in to_process.items():\n",
        "        answer += fun(k, v)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "gW1qhjJWwQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "U5AMZtaev11j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 sentences will be used for eval, the rest for training\n",
        "X_eval = train_sent[:30]\n",
        "y_eval = train_ners[:30]\n",
        "X_train = train_sent[30:]\n",
        "y_train = train_ners[30:]\n",
        "\n",
        "# transforming the training set with the function above\n",
        "training_set = list()\n",
        "for sent, ner in zip(X_train, y_train):\n",
        "    training_set += fun(sent, ner)"
      ],
      "metadata": {
        "id": "eh4Cjt0twmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_lg\")\n",
        "\n",
        "# disabling all pipes except for `ner`\n",
        "disabled_pipes = list()\n",
        "for pipe_name in nlp.pipe_names:\n",
        "    if pipe_name != \"ner\":\n",
        "        nlp.disable_pipes(pipe_name)\n",
        "        disabled_pipes.append(pipe_name)\n",
        "\n",
        "optimizer = nlp.create_optimizer()\n",
        "\n",
        "n_epochs = 20\n",
        "batch = 16\n",
        "for i in range(n_epochs):\n",
        "    losses = {}\n",
        "    pbar = tqdm(total=len(training_set)//batch+1, position=0, leave=True)\n",
        "    pbar.set_description_str(f\"Epoch [{i+1}/{n_epochs}]\")\n",
        "\n",
        "    for j in range(len(training_set)//batch+1):\n",
        "        # creating a list of randomly selected examples from training set\n",
        "        example = list()\n",
        "        for item in random.sample(training_set, batch):\n",
        "            entities = [(x[1], x[2]+1, x[3]) for x in item[1]]\n",
        "            doc = nlp.make_doc(item[0])\n",
        "            example.append(Example.from_dict(doc, {\"entities\": entities}))\n",
        "        # training nlp ner on these examples\n",
        "        try: nlp.update(example, drop=0.3, losses=losses, sgd=optimizer)\n",
        "        except: pass\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix_str(f\"Loss: {round(losses['ner']/(j+1), 2)}\")\n",
        "\n",
        "# enabling the pipes back\n",
        "for pipe_name in disabled_pipes:\n",
        "    nlp.enable_pipe(pipe_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xf17IhDwcyW",
        "outputId": "e1d869fa-2963-48db-8693-b1d0d05e9ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [48:15<00:00, 44.55s/it]\n",
            "Epoch [1/20]: 100%|██████████| 303/303 [00:58<00:00,  5.14it/s, Loss: 131.28]\n",
            "Epoch [2/20]: 100%|██████████| 303/303 [00:57<00:00,  5.24it/s, Loss: 95.43]\n",
            "Epoch [3/20]: 100%|██████████| 303/303 [00:56<00:00,  5.40it/s, Loss: 81.54]\n",
            "Epoch [4/20]: 100%|██████████| 303/303 [00:57<00:00,  5.24it/s, Loss: 76.22]\n",
            "Epoch [5/20]: 100%|██████████| 303/303 [00:55<00:00,  5.46it/s, Loss: 69.49]\n",
            "Epoch [6/20]: 100%|██████████| 303/303 [01:01<00:00,  4.89it/s, Loss: 75.62]\n",
            "Epoch [7/20]: 100%|██████████| 303/303 [00:55<00:00,  5.45it/s, Loss: 62.62]\n",
            "Epoch [8/20]: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s, Loss: 63.36]\n",
            "Epoch [9/20]: 100%|██████████| 303/303 [00:57<00:00,  5.27it/s, Loss: 59.25]\n",
            "Epoch [10/20]: 100%|██████████| 303/303 [00:59<00:00,  5.08it/s, Loss: 61.21]\n",
            "Epoch [11/20]: 100%|██████████| 303/303 [00:59<00:00,  5.10it/s, Loss: 58.03]\n",
            "Epoch [12/20]: 100%|██████████| 303/303 [01:02<00:00,  4.86it/s, Loss: 54.31]\n",
            "Epoch [13/20]: 100%|██████████| 303/303 [00:57<00:00,  5.29it/s, Loss: 53.22]\n",
            "Epoch [14/20]: 100%|██████████| 303/303 [00:58<00:00,  5.14it/s, Loss: 52.45]\n",
            "Epoch [15/20]: 100%|██████████| 303/303 [00:56<00:00,  5.34it/s, Loss: 49.23]\n",
            "Epoch [16/20]: 100%|██████████| 303/303 [00:59<00:00,  5.07it/s, Loss: 47.88]\n",
            "Epoch [17/20]: 100%|██████████| 303/303 [00:55<00:00,  5.43it/s, Loss: 44.81]\n",
            "Epoch [18/20]: 100%|██████████| 303/303 [01:02<00:00,  4.87it/s, Loss: 49.75]\n",
            "Epoch [19/20]: 100%|██████████| 303/303 [00:57<00:00,  5.29it/s, Loss: 42.73]\n",
            "Epoch [20/20]: 100%|██████████| 303/303 [01:00<00:00,  5.01it/s, Loss: 41.83]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finy-tuning"
      ],
      "metadata": {
        "id": "UUYosoE-BNZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_f1(pred, true):\n",
        "    \"\"\"\n",
        "    Evaluation function for NER. Calculates the f1-score like this:\n",
        "    true positives are the entities correctly classified\n",
        "    false positives are predicted entities that aren't in ground truth\n",
        "    false negatives are true entities that weren't predicted.\n",
        "    The f1-score is then calculated as usual\n",
        "\n",
        "    Format of input variables:\n",
        "    List of tuples (<word>, <start char idx>, <end char idx (inclusive)>, <type>)\n",
        "    \"\"\"\n",
        "    true = set(true)\n",
        "    pred = set(pred)\n",
        "\n",
        "    true_positive = len(true & pred)\n",
        "    false_positive = len(pred - true)\n",
        "    false_negative = len(true - pred)\n",
        "\n",
        "    if not true_positive:\n",
        "        return 0\n",
        "\n",
        "    recall = true_positive / (true_positive+false_positive)\n",
        "    precision = true_positive / (true_positive+false_negative)\n",
        "\n",
        "    return 2*precision*recall / (precision+recall)"
      ],
      "metadata": {
        "id": "unHsrtpqBKiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_to_test = 30\n",
        "max_depth = 7\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"\"] + [f\"depth={x}\" for x in range(max_depth)]\n",
        "\n",
        "pbar = tqdm(total=max_depth*items_to_test, position=0)\n",
        "\n",
        "row = [\"F1-score\"]\n",
        "\n",
        "for depth in range(max_depth):\n",
        "    total_f1 = 0\n",
        "\n",
        "    for idx in range(items_to_test):\n",
        "        pbar.set_postfix_str(f\"Depth={depth}, item={idx+1}, f1-score: {round(100*total_f1/(idx+1), 3)}%\")\n",
        "\n",
        "        pred = get_entities_spacy(train_sent[idx], recursion_depth=depth)\n",
        "        true = train_ners[idx]\n",
        "\n",
        "        f1_score = eval_f1(pred, true)\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    row.append(f\"{round(100*total_f1/items_to_test, 2)}%\")\n",
        "table.add_row(row)\n",
        "\n",
        "print()\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W86Zpk-twSj_",
        "outputId": "9df320a6-cc04-4205-c74f-eed637c4ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/210 [00:03<?, ?it/s, Depth=0, item=1, f1-score: 0.0%]\n",
            "100%|██████████| 210/210 [01:23<00:00,  2.86it/s, Depth=6, item=30, f1-score: 68.58%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "+----------+---------+---------+---------+---------+---------+---------+---------+\n",
            "|          | depth=0 | depth=1 | depth=2 | depth=3 | depth=4 | depth=5 | depth=6 |\n",
            "+----------+---------+---------+---------+---------+---------+---------+---------+\n",
            "| F1-score |  66.36% |  70.73% |  71.14% |  71.21% |  71.19% |  71.19% |  71.19% |\n",
            "+----------+---------+---------+---------+---------+---------+---------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "gMhtjItav5dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = tqdm(\n",
        "    total=len(test_sent), position=0\n",
        ")\n",
        "\n",
        "depth = 5\n",
        "\n",
        "with open(\"test.jsonl\", \"w\") as f:\n",
        "    for s, idx in zip(test_sent, test_idx):\n",
        "        ners = get_entities_spacy(s, depth)\n",
        "        ners = [ [x[1], x[2], x[3]] for x in ners]\n",
        "        ners = {\"ners\": ners, \"id\": idx}\n",
        "        ners = json.dumps(ners)\n",
        "        f.write(f\"{ners}\\n\")\n",
        "        pbar.update(1)\n",
        "\n",
        "print()\n",
        "!zip test test.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr5Y1uswwTzV",
        "outputId": "0fd0a217-0a2f-4fe5-fdf0-200fa0ee1267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [01:15<00:00,  1.16s/it]\n",
            "100%|██████████| 65/65 [00:31<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "updating: test.jsonl (deflated 76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B6tBek8Bs8s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}